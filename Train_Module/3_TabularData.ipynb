{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_random(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "fix_random(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "num_rows, num_cols = df.shape\n",
    "print(\"Rows: \", num_rows)\n",
    "print(\"Columns: \", num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Null rows:\", df.shape[0] - df.dropna().shape[0])\n",
    "print(\"Duplicated rows:\", df.duplicated().sum())\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Year distribution\")\n",
    "sns.kdeplot(data=df[\"Year\"], fill=True, color=\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate indices in train/val/set\n",
    "# \"stratify=y\" makes sure to keep the classes proportions on the dataset (useful on imbalanced classes)\n",
    "# train, test = train_test_split(df, stratify=df[\"Year\"], test_size=0.3, random_state=random_state)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[\"Year\"]), df[\"Year\"], stratify=df[\"Year\"], test_size=0.3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Instantiate the RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=random_state, sampling_strategy='majority')\n",
    "\n",
    "# Fit the RandomUnderSampler\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_resampled.shape, X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Year distribution after Under Sampling\")\n",
    "sns.kdeplot(y_resampled, fill=True, color=\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.covariance import OAS\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"min-max\", preprocessing.MinMaxScaler()),\n",
    "    (\"lmax\", preprocessing.Normalizer(norm=\"max\")),\n",
    "    (\"lda\", LinearDiscriminantAnalysis(solver=\"eigen\", shrinkage=None, covariance_estimator=OAS()))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data using the pipeline\n",
    "X_train_t = pipeline.transform(X_train)\n",
    "X_test_t = pipeline.transform(X_test)\n",
    "\n",
    "X_train_t = pd.DataFrame(X_train_t)\n",
    "X_test_t = pd.DataFrame(X_test_t)\n",
    "\n",
    "train_t = pd.DataFrame(X_train_t)\n",
    "test_t = pd.DataFrame(X_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t['Year'] = y_train\n",
    "test_t['Year'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['Year']\n",
    "continous_cols = list(train_t.columns)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import (\n",
    "    TabNetModelConfig, \n",
    "    TabTransformerConfig\n",
    ")\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig, ExperimentConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig(\n",
    "    target=target,\n",
    "    continuous_cols=continous_cols,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "head_config = LinearHeadConfig(\n",
    "    layers=\"\", # No additional layer in head, just a mapping layer to output_dim\n",
    "    #dropout=0.1,\n",
    "    initialization=\"kaiming\"\n",
    ").__dict__ # Convert to dict to pass to the model config (OmegaConf doesn't accept objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.005, 0.05]\n",
    "batch_sizes = [256, 512]\n",
    "#batch_sizes = [256, 512, 1024]\n",
    "\n",
    "#virtual_batch_sizes = [128, 256]\n",
    "n_epochs = [50, 100]\n",
    "# Dimension of the prediction  layer\n",
    "n_d = [16]\n",
    "#n_d = [16, 32]\n",
    "#n_d = [8, 16, 32, 64]\n",
    "# Dimension of the attention  layer\n",
    "n_a = [16]\n",
    "#n_a = [8, 16, 32, 64]\n",
    "# Number of successive steps in the network\n",
    "#n_steps = [3, 5]\n",
    "n_steps = [3, 5]\n",
    "\n",
    "# Number of independent GLU layer in each GLU block\n",
    "# n_indipendent = [2, 3]\n",
    "# Coefficient for feature reusage in the masks. A value close to 1 will make mask selection least correlated between layers. Values range from 1.0 to 2.0.\n",
    "gamma = [1.3, 1.5]\n",
    "\n",
    "params = list(product(learning_rates, \n",
    "                      batch_sizes, \n",
    "                      #virtual_batch_sizes, \n",
    "                      n_epochs, \n",
    "                      n_d, \n",
    "                      n_a, \n",
    "                      n_steps, \n",
    "                      gamma))\n",
    "comb = (\n",
    "    len(learning_rates)\n",
    "    * len(batch_sizes)\n",
    "    #* len(virtual_batch_sizes)\n",
    "    * len(n_epochs)\n",
    "    * len(n_d)\n",
    "    * len(n_a)\n",
    "    * len(n_steps)\n",
    "    * len(gamma)\n",
    ")\n",
    "print(\"Number of combinations: \", comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mse = float(\"inf\")\n",
    "best_model = None\n",
    "best_params = None\n",
    "iter = 0\n",
    "results = pd.DataFrame(columns=['loss', 'r2', 'learning_rate', 'batch_size', 'epochs', 'n_d', 'n_a', 'n_steps', 'gamma'])\n",
    "\n",
    "\n",
    "for learning_rate, batch_size, epochs, n_d, n_a, n_steps, gamma in params:\n",
    "    iter += 1\n",
    "    print(f\"\\nIteration: {iter} of {comb}\")\n",
    "    print(\n",
    "        f\"Configuration: learning_rate={learning_rate}, batch_size={batch_size}, n_epochs={epochs}, n_d={n_d}, n_a={n_a}, n_steps={n_steps}, gamma={gamma}\"\n",
    "    )\n",
    "    trainer_config = TrainerConfig(\n",
    "        batch_size=batch_size,\n",
    "        max_epochs=epochs,\n",
    "        early_stopping_patience=10,\n",
    "    )\n",
    "\n",
    "    model_config = TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        learning_rate=learning_rate,\n",
    "        head=\"LinearHead\",  # Linear Head\n",
    "        head_config=head_config,  # Linear Head Config\n",
    "        #virtual_batch_size=virtual_batch_size,\n",
    "        n_d=n_d,\n",
    "        n_a=n_a,\n",
    "        n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        # n_independent=n_independent\n",
    "        )\n",
    "\n",
    "    tabular_model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=model_config,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config,\n",
    "    )\n",
    "\n",
    "    tabular_model.fit(train=train_t)\n",
    "    tabular_model.evaluate(test_t)\n",
    "\n",
    "    y_pred= tabular_model.predict(X_test_t)\n",
    "    mse= mean_squared_error(y_test, y_pred)\n",
    "    r2= r2_score(y_test, y_pred)\n",
    "\n",
    "    print(\"MSE: \", mse)\n",
    "    print(\"R2: \", r2)\n",
    "\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_model = copy.deepcopy(tabular_model)\n",
    "        best_params = (learning_rate, batch_size, \n",
    "                       #virtual_batch_size, \n",
    "                       epochs, n_d, n_a, n_steps, gamma, #n_independent\n",
    "                       )\n",
    "        print(\"Best model updated\")\n",
    "    \n",
    "    config = [mse, r2, learning_rate, batch_size, epochs, n_d, n_a, n_steps, gamma]\n",
    "    results.loc[len(results)] = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by='r2', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by='r2', ascending=False).to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_config = TabTransformerConfig(\n",
    "#     task=\"regression\",\n",
    "#     learning_rate = 1e-3,\n",
    "#     head = \"LinearHead\", #Linear Head\n",
    "#     head_config = head_config, # Linear Head Config\n",
    "# )\n",
    "\n",
    "# tabular_model = TabularModel(\n",
    "#     data_config=data_config,\n",
    "#     model_config=model_config,\n",
    "#     optimizer_config=optimizer_config,\n",
    "#     trainer_config=trainer_config,\n",
    "# )\n",
    "\n",
    "# tabular_model.fit(train=train_t)\n",
    "# tabular_model.evaluate(test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred= tabular_model.predict(X_test_t)\n",
    "# mse= mean_squared_error(y_test, y_pred)\n",
    "# r2= r2_score(y_test, y_pred)\n",
    "\n",
    "# print(\"MSE: \", mse)\n",
    "# print(\"R2: \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.005]\n",
    "batch_sizes = [256, 512]\n",
    "n_epochs = [50, 100]\n",
    "virtual_batch_sizes = [128, 256]\n",
    "\n",
    "params = list(product(learning_rates, batch_sizes, n_epochs,virtual_batch_sizes ))\n",
    "comb = (len(learning_rates)* len(batch_sizes) * len(n_epochs) * len(virtual_batch_sizes)\n",
    "        )\n",
    "print(\"Number of combinations: \", comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mse_tt = float(\"inf\")\n",
    "best_model_tt = None\n",
    "best_params_tt = None\n",
    "iter = 0\n",
    "results_tt = pd.DataFrame(columns=['loss', 'r2', 'learning_rate', 'epochs', 'batch_size', 'virtual_batch_size'])\n",
    "\n",
    "for learning_rate, batch_size, epochs, virtual_batch_size in params:\n",
    "    iter += 1\n",
    "    print(f\"\\nIteration: {iter} of {comb}\")\n",
    "    trainer_config = TrainerConfig(\n",
    "        batch_size=batch_size,\n",
    "        max_epochs=epochs,\n",
    "        early_stopping_patience=5,\n",
    "        load_best=True\n",
    "    )\n",
    "\n",
    "    model_config = TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        learning_rate=learning_rate,\n",
    "        head=\"LinearHead\",  # Linear Head\n",
    "        head_config=head_config,  # Linear Head Config\n",
    "        )\n",
    "\n",
    "    tabular_model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=model_config,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    tabular_model.fit(train=train_t)\n",
    "    tabular_model.evaluate(test_t)\n",
    "\n",
    "    y_pred= tabular_model.predict(X_test_t)\n",
    "    mse= mean_squared_error(y_test, y_pred)\n",
    "    r2= r2_score(y_test, y_pred)\n",
    "\n",
    "    config = [mse, r2, learning_rate, epochs, batch_size, virtual_batch_size]\n",
    "\n",
    "    print(\"MSE: \", mse)\n",
    "    print(\"R2: \", r2)\n",
    "\n",
    "    if mse < best_mse_tt:\n",
    "        best_mse_tt = mse\n",
    "        best_model_tt = copy.deepcopy(tabular_model)\n",
    "        best_params_tt = (learning_rate, batch_size, \n",
    "                       virtual_batch_size, \n",
    "                       epochs\n",
    "                       )\n",
    "        print(\"Best model updated\")\n",
    "\n",
    "    results_tt.loc[len(results_tt)] = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tt.sort_values(by='r2', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tt.sort_values(by='r2', ascending=False).to_csv('out_tab_transformers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.366600265340756"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
