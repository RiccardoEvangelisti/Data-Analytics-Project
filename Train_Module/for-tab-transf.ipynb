{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = 42\n",
    "preproc = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_random(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "fix_random(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "num_rows, num_cols = df.shape\n",
    "print(\"Rows: \", num_rows)\n",
    "print(\"Columns: \", num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Null rows:\", df.shape[0] - df.dropna().shape[0])\n",
    "print(\"Duplicated rows:\", df.duplicated().sum())\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate indices in train/val/set\n",
    "# \"stratify=y\" makes sure to keep the classes proportions on the dataset (useful on imbalanced classes)\n",
    "train, test = train_test_split(df, stratify=df[\"Year\"], test_size=0.3, random_state=random_state)\n",
    "val, test = train_test_split(test, stratify=test[\"Year\"], test_size=(1 / 3), random_state=random_state)\n",
    "\n",
    "X_train = train.drop(columns=[\"Year\"])\n",
    "y_train = train[\"Year\"]\n",
    "\n",
    "X_val = val.drop(columns=[\"Year\"])\n",
    "y_val = val[\"Year\"]\n",
    "\n",
    "X_test = test.drop(columns=[\"Year\"])\n",
    "y_test = test[\"Year\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"std\", preprocessing.StandardScaler()),\n",
    "        (\"l2\", preprocessing.Normalizer(norm=\"l2\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if preproc==True:\n",
    "    # Fit the pipeline to the data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Transform the data using the pipeline\n",
    "    X_train = pipeline.transform(X_train)\n",
    "    X_test = pipeline.transform(X_test)\n",
    "    X_val = pipeline.transform(X_val)\n",
    "\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    X_val = pd.DataFrame(X_val)\n",
    "\n",
    "    train = pd.DataFrame(X_train)\n",
    "    test = pd.DataFrame(X_test)\n",
    "    val = pd.DataFrame(X_val)\n",
    "    \n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True) \n",
    "    y_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train[\"Year\"] = y_train\n",
    "    test[\"Year\"] = y_test\n",
    "    val[\"Year\"] = y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [\"Year\"]\n",
    "continous_cols = list(train.columns)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import TabNetModelConfig, TabTransformerConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig, ExperimentConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig(target=target, continuous_cols=continous_cols, num_workers=0)\n",
    "\n",
    "optimizer_config = OptimizerConfig(\n",
    "    optimizer=\"AdamW\",\n",
    "    lr_scheduler=\"ReduceLROnPlateau\",\n",
    "    lr_scheduler_params={\"patience\": 9, \"threshold\": 1, \"threshold_mode\": \"abs\"},\n",
    ")\n",
    "\n",
    "head_config = LinearHeadConfig(\n",
    "    layers=\"\",  # No additional layer in head, just a mapping layer to output_dim\n",
    "    # dropout=0.2,\n",
    "    initialization=\"kaiming\",\n",
    ").__dict__  # Convert to dict to pass to the model config (OmegaConf doesn't accept objects)\n",
    "\n",
    "if preproc == False:\n",
    "    experiment_config = ExperimentConfig(\n",
    "        project_name=\"TabTransformer\",\n",
    "        run_name=\"TabTransformer-raw\",\n",
    "        log_target=\"tensorboard\",\n",
    "    )\n",
    "else:\n",
    "    experiment_config = ExperimentConfig(\n",
    "        project_name=\"TabTransformer\",\n",
    "        run_name=\"TabTransformer-preproc\",\n",
    "        log_target=\"tensorboard\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_batch_sizes = [64, 128]\n",
    "batch_sizes = [256, 512]\n",
    "n_epochs = [100]\n",
    "learning_rates = [0.01]\n",
    "num_heads = [8]  # default is 8\n",
    "num_attn_blocks = [6]  # default is 6\n",
    "transformer_activation = ['ReLU', 'LeakyReLU', 'GEGLU', 'ReGLU']\n",
    "\n",
    "params = list(\n",
    "    product(\n",
    "        learning_rates, batch_sizes, n_epochs, virtual_batch_sizes, num_heads, num_attn_blocks, transformer_activation\n",
    "    )\n",
    ")\n",
    "\n",
    "comb = (\n",
    "    len(learning_rates)\n",
    "    * len(batch_sizes)\n",
    "    * len(n_epochs)\n",
    "    * len(virtual_batch_sizes)\n",
    "    * len(num_heads)\n",
    "    * len(num_attn_blocks)\n",
    "    * len(transformer_activation)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Number of combinations: \", comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "best_mse_tt = float(\"inf\")\n",
    "best_model_tt = None\n",
    "best_params_tt = None\n",
    "iter = 0\n",
    "results_tt = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"loss\",\n",
    "        \"r2\",\n",
    "        \"learning_rate\",\n",
    "        \"epochs\",\n",
    "        \"batch_size\",\n",
    "        \"virtual_batch_size\",\n",
    "        \"num_heads\",\n",
    "        \"num_attn_blocks\",\n",
    "        \"transformer_activation\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for learning_rate, batch_size, epochs, virtual_batch_size, num_heads, num_attn_blocks, transformer_activation in params:\n",
    "    iter += 1\n",
    "    print(f\"\\nIteration: {iter} of {comb}\")\n",
    "    trainer_config = TrainerConfig(batch_size=batch_size, max_epochs=epochs, early_stopping_patience=10, load_best=True)\n",
    "\n",
    "    model_config = TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",  # Linear Head\n",
    "        head_config=head_config,  # Linear Head Config\n",
    "        loss=\"MSELoss\",\n",
    "        seed=random_state,\n",
    "        learning_rate=learning_rate,\n",
    "        virtual_batch_size=virtual_batch_size,\n",
    "        num_heads=num_heads,\n",
    "        num_attn_blocks=num_attn_blocks,\n",
    "        ff_hidden_multiplier=64,\n",
    "        transformer_activation=transformer_activation\n",
    "    )\n",
    "\n",
    "    tabular_model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=model_config,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config,\n",
    "        experiment_config=experiment_config,\n",
    "    )\n",
    "\n",
    "    tabular_model.fit(train=train, validation=val)\n",
    "    tabular_model.evaluate(test)\n",
    "\n",
    "    y_pred = tabular_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    config = [mse, r2, learning_rate, batch_size, epochs, virtual_batch_size, num_heads, num_attn_blocks, transformer_activation]\n",
    "\n",
    "    print(\"MSE: \", mse)\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"R2: \", r2)\n",
    "\n",
    "    if mse < best_mse_tt:\n",
    "        best_mse_tt = mse\n",
    "        best_model_tt = copy.deepcopy(tabular_model)\n",
    "        best_params_tt = (learning_rate, batch_size, epochs, virtual_batch_size, num_heads, num_attn_blocks, transformer_activation)\n",
    "        print(\"Best model updated\")\n",
    "\n",
    "    results_tt.loc[len(results_tt)] = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tt.sort_values(by=\"r2\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if preproc==True:\n",
    "    results_tt.sort_values(by=\"r2\", ascending=False).to_csv('tabtransformer-preproc.csv')\n",
    "else:\n",
    "    results_tt.sort_values(by=\"r2\", ascending=False).to_csv('tabtransformer-raw.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
