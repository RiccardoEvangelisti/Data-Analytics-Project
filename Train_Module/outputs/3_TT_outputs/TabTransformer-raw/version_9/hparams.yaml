target:
- Year
continuous_cols:
- Year
- S0
- S1
- S2
- S3
- S4
- S5
- S6
- S7
- S8
- S9
- S10
- S11
- S12
- S13
- S14
- S15
- S16
- S17
- S18
- S19
- S20
- S21
- S22
- S23
- S24
- S25
- S26
- S27
- S28
- S29
- S30
- S31
- S32
- S33
- S34
- S35
- S36
- S37
- S38
- S39
- S40
- S41
- S42
- S43
- S44
- S45
- S46
- S47
- S48
- S49
- S50
- S51
- S52
- S53
- S54
- S55
- S56
- S57
- S58
- S59
- S60
- S61
- S62
- S63
- S64
- S65
- S66
- S67
- S68
- S69
- S70
- S71
- S72
- S73
- S74
- S75
- S76
- S77
- S78
- S79
- S80
- S81
- S82
- S83
- S84
- S85
- S86
- S87
- S88
categorical_cols: []
date_columns: []
encode_date_columns: true
validation_split: 0.2
continuous_feature_transform: null
normalize_continuous_features: true
quantile_noise: 0
num_workers: 0
pin_memory: true
handle_unknown_categories: true
handle_missing_values: true
task: regression
head: LinearHead
head_config:
  layers: ''
  activation: ReLU
  dropout: 0.0
  use_batch_norm: false
  initialization: kaiming
embedding_dims: []
embedding_dropout: 0.0
batch_norm_continuous_input: true
learning_rate: 0.01
loss: MSELoss
metrics:
- mean_squared_error
metrics_prob_input:
- false
metrics_params:
- {}
target_range: null
virtual_batch_size: 64
seed: 42
_module_src: models.tab_transformer
_model_name: TabTransformerModel
_backbone_name: TabTransformerBackbone
_config_name: TabTransformerConfig
input_embed_dim: 32
embedding_initialization: kaiming_uniform
embedding_bias: false
share_embedding: false
share_embedding_strategy: fraction
shared_embedding_fraction: 0.25
num_heads: 8
num_attn_blocks: 6
transformer_head_dim: null
attn_dropout: 0.1
add_norm_dropout: 0.1
ff_dropout: 0.1
ff_hidden_multiplier: 64
transformer_activation: GEGLU
batch_size: 256
data_aware_init_batch_size: 2000
fast_dev_run: false
max_epochs: 100
min_epochs: 1
max_time: null
accelerator: auto
devices: -1
devices_list: null
accumulate_grad_batches: 1
auto_lr_find: false
auto_select_gpus: true
check_val_every_n_epoch: 1
gradient_clip_val: 0.0
overfit_batches: 0.0
deterministic: false
profiler: null
early_stopping: valid_loss
early_stopping_min_delta: 0.001
early_stopping_mode: min
early_stopping_patience: 10
early_stopping_kwargs: {}
checkpoints: valid_loss
checkpoints_path: saved_models
checkpoints_every_n_epochs: 1
checkpoints_name: null
checkpoints_mode: min
checkpoints_save_top_k: 1
checkpoints_kwargs: {}
load_best: true
track_grad_norm: -1
progress_bar: rich
precision: 32
trainer_kwargs: {}
project_name: TabTransformer
run_name: TabTransformer-raw
exp_watch: null
log_target: tensorboard
log_logits: false
exp_log_freq: 100
optimizer: AdamW
optimizer_params: {}
lr_scheduler: ReduceLROnPlateau
lr_scheduler_params:
  patience: 9
  threshold: 1
  threshold_mode: abs
lr_scheduler_monitor_metric: valid_loss
categorical_dim: 0
continuous_dim: 90
output_dim: 1
categorical_cardinality: []
embedded_cat_dim: 0
