{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:38.127067100Z",
     "start_time": "2023-06-22T17:05:38.098187500Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter  # log writer to visualize the loss functions\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility, fix all the seeds\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da valutare come vogliamo disporre le directory in base al preprocess che facciamo. Ho lasciato una versione che differenzia in base al preproc per averla già pronta da modificare nel caso serva\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# versione Rick\n",
    "base_dir = \"NN_outputs/\"\n",
    "model_dir = base_dir + \"models/\"\n",
    "runs_dir = base_dir + \"runs/\"\n",
    "\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "shutil.rmtree(runs_dir, ignore_errors=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(runs_dir, exist_ok=True)\n",
    "\n",
    "######################\n",
    "# if not os.path.exists('results'):\n",
    "#     os.mkdir('results')\n",
    "\n",
    "# if not os.path.exists('results/pca'):\n",
    "#     os.mkdir('results/pca')\n",
    "\n",
    "# if not os.path.exists('results/no_pca'):\n",
    "#     os.mkdir('results/no_pca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    # Save X and y as Tensors, accordingly to the type of the data\n",
    "    # https://pytorch.org/docs/stable/tensors.html\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "\n",
    "        # Useful attributes\n",
    "        self.num_features = X.shape[1]\n",
    "        self.num_classes = len(np.unique(y))\n",
    "\n",
    "    # Dataset size\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    # Fetch a data sample (single sample or batch) for a given index/es\n",
    "    # (if the dataset is not in memory, it can read from file system and return the object)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the neural network architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:42.070805500Z",
     "start_time": "2023-06-22T17:05:42.065927700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Neural Network class\n",
    "# Extend the abstract class \"Module\"\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # Useful attributes\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Definition of Layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)  # input to hidden\n",
    "        self.fc2 = nn.Linear(self.hidden_size, num_classes)  # hidden to output\n",
    "\n",
    "        # Activation Function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # How layers are connected between them\n",
    "    # This even defines the graph of backpropagation\n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x)  # first layer\n",
    "        h = self.relu(h)  # activation function\n",
    "        output = self.fc2(h)  # second layer\n",
    "        return output\n",
    "\n",
    "    def _get_name(self):\n",
    "        return \"FeedForward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:42.116054200Z",
     "start_time": "2023-06-22T17:05:42.087104900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for the training process\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,  # instance of class to train\n",
    "    criterion,  # instance of loss function\n",
    "    optimizer,  # instance of optimizer\n",
    "    epochs,  # number of\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device,  # to train on\n",
    "    log_writer,\n",
    "    log_name,\n",
    "):\n",
    "    n_iter = 0\n",
    "    best_valid_loss = float(\"inf\")  # initialized to worst possible value\n",
    "    \n",
    "    #variabili per early stopping \n",
    "    #########################################DA VEDERE\n",
    "    patience = 10\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # EPOCHS\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # activate training mode (for BatchNorm or Dropout)\n",
    "\n",
    "        # BATCHES\n",
    "        for data, targets in train_loader:  # get_item from MyDataset class (single item or batch)\n",
    "            data, targets = data.to(device), targets.to(device)  # move data and targets to cpu/gpu\n",
    "\n",
    "            optimizer.zero_grad()  # gradient to zero\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(data)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred, targets)\n",
    "            log_writer.add_scalar(\"Loss/train\", loss, n_iter)  # plot the batches\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_iter += 1\n",
    "\n",
    "        # Valuation\n",
    "        y_test, _, y_pred = test_model(model, val_loader, device)\n",
    "        loss_val = criterion(y_pred, y_test)\n",
    "        log_writer.add_scalar(\"Loss/val\", loss_val, epoch)  # plot the epochs\n",
    "\n",
    "        # Save the model with best loss through the epochs\n",
    "        if loss_val.item() < best_valid_loss:\n",
    "            best_valid_loss = loss_val.item()\n",
    "            torch.save(model.state_dict(), model_dir + log_name)\n",
    "        # #early stopping \n",
    "        # #####################################################DA VEDERE\n",
    "        # else:\n",
    "        #     epochs_no_improve += 1\n",
    "        #     if epochs_no_improve == patience:\n",
    "        #         print('Early stopping!')\n",
    "        #         break\n",
    "            \n",
    "### best model me lo salvo in qualche modo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:42.116054200Z",
     "start_time": "2023-06-22T17:05:42.106079700Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(model: nn.Module, data_loader: DataLoader, device) -> tuple[Tensor, Tensor, Tensor]:\n",
    "    \"\"\"return:\n",
    "    - y_test - true lables\n",
    "    - y_pred_c - has 1 column, where each element is the predicted lable with bigger probability among the \"c\" predicted\n",
    "    - y_pred - has \"c\" columns as the number of classes of the test set\n",
    "    \"\"\"\n",
    "    model.eval()  # activate evaluation mode (for BatchNorm or Dropout)\n",
    "\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    # ## oppure\n",
    "    # y_pred = torch.tensor([]).to(device)\n",
    "    # y_true = torch.tensor([]).to(device)\n",
    "\n",
    "    for data, targets in data_loader:\n",
    "        data, targets = data.to(device), targets.to(device)  # move data and targets to cpu/gpu\n",
    "        \n",
    "        y_pred.append(model(data))  # accumulate predictions\n",
    "        y_test.append(targets)  # accumulate labels\n",
    "\n",
    "    y_test = torch.stack(y_test).squeeze()  # it's one column (each row is a different sample)\n",
    "    y_pred = torch.stack(\n",
    "        y_pred\n",
    "    ).squeeze()  # there are \"c\" columns as the number of classes. Each column is the probability (as float number) to that class (each row is a different sample)\n",
    "    y_pred_c = y_pred.argmax(\n",
    "        dim=1, keepdim=True\n",
    "    ).squeeze()  # return max position of prediction array: that is the class I will associate with the sample\n",
    "    return y_test, y_pred_c, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:40.247570400Z",
     "start_time": "2023-06-22T17:05:40.244573800Z"
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:42.115056600Z",
     "start_time": "2023-06-22T17:05:42.070805500Z"
    }
   },
   "outputs": [],
   "source": [
    "# valutare hidden_size\n",
    "hidden_size = [128, 256, 512]  # 128\n",
    "# oppure\n",
    "# hidden_sizes =  [256, 512, 1024]\n",
    "\n",
    "# da valutare batch_size\n",
    "batch_size = [32, 64]  # 32\n",
    "# oppure\n",
    "# batch_size = [8,16,32]\n",
    "\n",
    "# abbiamo detto che non facciamo drop out\n",
    "dropout_p = [0.2, 0.3]  # 0.2\n",
    "\n",
    "depth = [3, 4, 5]\n",
    "\n",
    "# da valutare learning_rate\n",
    "learning_rate = [0.001, 0.1]\n",
    "# oppure\n",
    "# learning_rate = [0.01, 0.001]\n",
    "# learning_rate = [0.1, 0.01, 0.001]\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "hyperparams = product(hidden_size, batch_size, dropout_p, depth, learning_rate)\n",
    "print(\"Number of combinations:\", len(list(hyperparams)))\n",
    "\n",
    "############# qual è la differenza? #######################\n",
    "# hyperparameters = itertools.product(hidden_sizes, depth, nums_epochs, batch_sizes, learning_rate, step_size_lr_decay, momentum)\n",
    "# n_comb = len(hidden_sizes)*len(depth)*len(nums_epochs)*len(batch_sizes)*len(learning_rate)*len(step_size_lr_decay)*len(momentum)\n",
    "# print (f'Number of hyperparameter combinations: {n_comb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA LOADING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:40.242589800Z",
     "start_time": "2023-06-22T17:05:38.117070300Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "num_rows, num_cols = df.shape\n",
    "print(\"Rows: \", num_rows)\n",
    "print(\"Columns: \", num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Null rows:\", df.shape[0] - df.dropna().shape[0])\n",
    "print(\"Duplicated rows:\", df.duplicated().sum())\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:40.565714200Z",
     "start_time": "2023-06-22T17:05:40.248568300Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_X = df.iloc[:, 1:]\n",
    "df_y = df.iloc[:, 0]\n",
    "indices = np.arange(df_X.shape[0])  # useful later to split the data in train/val/test\n",
    "\n",
    "# Separate indices in train/val/set\n",
    "# \"stratify=y\" makes sure to keep the classes proportions on the dataset (useful on imbalanced classes)\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, stratify=df_y, random_state=random_state)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.2, stratify=df_y[train_idx], random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:42.056143500Z",
     "start_time": "2023-06-22T17:05:40.565714200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#VEDERE COME PREPROCESSARE\n",
    "#visto che sono tutte gaussiane\n",
    "#possiamo normalizzare, batch normalizzare, \n",
    "#standardizzare + scaler che tolga gli outlier\n",
    "# altro?? da vedere "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:42.066925800Z",
     "start_time": "2023-06-22T17:05:42.055145700Z"
    }
   },
   "outputs": [],
   "source": [
    "## ciclare sotto su batch size \n",
    "# DataLoaders\n",
    "my_dataset = MyDataset(df_X, df_y)\n",
    "\n",
    "train_subset = Subset(my_dataset, train_idx)\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_subset = Subset(my_dataset, val_idx)\n",
    "val_loader = DataLoader(val_subset, batch_size=1)\n",
    "\n",
    "test_subset = Subset(my_dataset, test_idx)\n",
    "test_loader = DataLoader(test_subset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model, Criterion, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random(random_state)\n",
    "\n",
    "hidden_size = 32\n",
    "model = NeuralNetwork(my_dataset.num_features, hidden_size, my_dataset.num_classes)\n",
    "model.to(device)  # move the NN to device\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "log_writer = SummaryWriter(runs_dir + model._get_name())  # Start tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "\n",
    "Run Tensorboard from the command line:\n",
    "\n",
    "> tensorboard --logdir nn/runs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ciclare sui parametri e inserire la roba nella cella sotto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T18:29:14.660201100Z",
     "start_time": "2023-06-22T18:29:14.586831500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test before the training\n",
    "y_test, y_pred_c, _ = test_model(model, test_loader, device)\n",
    "acc = (y_test == y_pred_c).float().sum() / y_test.shape[0]\n",
    "print(\"Accuracy before training:\", acc.cpu().numpy())\n",
    "\n",
    "\n",
    "# Train\n",
    "train_model(model,criterion,optimizer,num_epochs,train_loader,val_loader,device,log_writer,model._get_name(),\n",
    ")\n",
    "\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(model_dir + model._get_name()))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Test after the training\n",
    "y_test, y_pred_c, _ = test_model(model, test_loader, device)\n",
    "acc = (y_test == y_pred_c).float().sum() / y_test.shape[0]\n",
    "print(\"Accuracy after training:\", acc.cpu().numpy())\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Close tensorboard writer after a training\n",
    "log_writer.flush()\n",
    "log_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T18:29:15.408534700Z",
     "start_time": "2023-06-22T18:29:14.660201100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.regplot(x=y_true, y=y_pred, scatter_kws={\"alpha\": 0.1})  ## alpha ???\n",
    "plt.xlabel(\"True year\")\n",
    "plt.ylabel(\"Predicted year\")\n",
    "plt.title(\"Predicted vs True year\")\n",
    "plt.show()\n",
    "\n",
    "# Distribution of predicted years\n",
    "plt.figure(figsize=(10, 10))\n",
    "y_pred = np.array(y_pred)\n",
    "sns.kdeplot(y_true)\n",
    "sns.kdeplot(y_pred.flatten())\n",
    "plt.legend([\"Predicted years\", \"True years\"])\n",
    "plt.xlabel(\"Predicted years\")\n",
    "plt.title(\"Distribution of predicted years\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
