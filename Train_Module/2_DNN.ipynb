{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:38.127067100Z",
     "start_time": "2023-06-22T17:05:38.098187500Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from itertools import product\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"2_NN_outputs/\"\n",
    "model_dir = base_dir + \"models/\"\n",
    "runs_dir = base_dir + \"runs/\"\n",
    "\n",
    "# shutil.rmtree(model_dir, ignore_errors=True)\n",
    "# shutil.rmtree(runs_dir, ignore_errors=True)\n",
    "# os.makedirs(model_dir, exist_ok=True)\n",
    "# os.makedirs(runs_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility, fix all the seeds\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "    Args:\n",
    "        seed: the seed to use.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    # Save X and y as Tensors, accordingly to the type of the data\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32, device=device)\n",
    "\n",
    "        self.num_features = X.shape[1]\n",
    "\n",
    "    # Dataset size\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:42.070805500Z",
     "start_time": "2023-06-22T17:05:42.065927700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Neural Network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, depth):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "            *[\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "            * (depth - 1)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_in = self.input_layer(x)\n",
    "        h_out = self.hidden_layers(h_in)\n",
    "        out = self.output(h_out)\n",
    "        return out\n",
    "\n",
    "    def _get_description(self):\n",
    "        return \"Linear(all same size)+ReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, delta=0):\n",
    "        self.patience: int = patience\n",
    "        self.delta: int = delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float(\"inf\")\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss <= (self.min_validation_loss - self.delta):\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            # print(\"\\tEarly stopper: {}/{} epochs\".format(self.counter, self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:42.116054200Z",
     "start_time": "2023-06-22T17:05:42.087104900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for the training process\n",
    "def train_model(\n",
    "    model: nn.Module,  # instance of class to train\n",
    "    criterion,  # instance of loss function\n",
    "    optimizer,  # instance of optimizer\n",
    "    epochs,  # number of\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device,  # to train on\n",
    "    log_writer,\n",
    "    log_name,\n",
    "):\n",
    "    n_iter = 0\n",
    "    best_valid_loss = float(\"inf\")  # initialized to worst possible value\n",
    "\n",
    "    early_stopper = EarlyStopper(patience=5, delta=0.1)\n",
    "\n",
    "    # EPOCHS\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # activate training mode (for BatchNorm or Dropout)\n",
    "\n",
    "        # BATCHES\n",
    "        for data, targets in train_loader:  # get_item from MyDataset class (single item or batch)\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # gradient to zero\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(data)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred.squeeze(), targets)  # reshape because MSELoss requires same dimensionality\n",
    "            log_writer.add_scalar(\"Loss train/batches\", loss, n_iter)  # plot the batches\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_iter += 1\n",
    "\n",
    "        # Valuation\n",
    "        y_test, y_pred = test_model(model, val_loader, device)\n",
    "        loss_val = criterion(y_pred.squeeze(), y_test)\n",
    "        log_writer.add_scalar(\"Loss val/epochs\", loss_val, epoch)  # plot the epochs\n",
    "\n",
    "        # Save the model with best loss through the epochs\n",
    "        if loss_val.item() < best_valid_loss:\n",
    "            best_valid_loss = loss_val.item()\n",
    "            torch.save(model.state_dict(), model_dir + log_name)\n",
    "\n",
    "        # Early Stopping\n",
    "        if early_stopper.early_stop(loss_val.item()):\n",
    "            print(\"Early stopped!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:42.116054200Z",
     "start_time": "2023-06-22T17:05:42.106079700Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(model: nn.Module, data_loader: DataLoader, device) -> tuple[Tensor, Tensor, Tensor]:\n",
    "    model.eval()  # activate evaluation mode (for BatchNorm or Dropout)\n",
    "\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    for data, targets in data_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        y_pred.append(model(data))  # accumulate predictions\n",
    "        y_test.append(targets)  # accumulate labels\n",
    "\n",
    "    y_test = torch.stack(y_test).squeeze()\n",
    "    y_pred = torch.stack(y_pred).squeeze()\n",
    "    return y_test, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:40.242589800Z",
     "start_time": "2023-06-22T17:05:38.117070300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:  252123\n",
      "Columns:  91\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\").drop_duplicates()\n",
    "print(\"Rows: \", df.shape[0])\n",
    "print(\"Columns: \", df.shape[1])\n",
    "\n",
    "df_X = df.iloc[:, 1:].values\n",
    "df_y = df.iloc[:, 0].values\n",
    "indices = np.arange(df_X.shape[0])\n",
    "\n",
    "# Separate indices in train/val/set\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, stratify=df_y, random_state=random_state)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.2, stratify=df_y[train_idx], random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T17:05:42.056143500Z",
     "start_time": "2023-06-22T17:05:40.565714200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.covariance import OAS\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([(\"raw\", None)])\n",
    "preprocess_name = \"RAW\"\n",
    "\n",
    "# pipeline = Pipeline(\n",
    "#     steps=[\n",
    "#         (\"min-max\", preprocessing.MinMaxScaler()),\n",
    "#         (\"lmax\", preprocessing.Normalizer(norm=\"max\")),\n",
    "#         (\"lda\", LinearDiscriminantAnalysis(solver=\"eigen\", shrinkage=None, covariance_estimator=OAS())),\n",
    "#     ]\n",
    "# )\n",
    "# preprocess_name = \"MinMax+Lmax+LDA\"\n",
    "\n",
    "# pipeline = Pipeline(\n",
    "#     [\n",
    "#         (\"min-max\", preprocessing.MinMaxScaler()),\n",
    "#         (\"lda\", LinearDiscriminantAnalysis(solver=\"eigen\", covariance_estimator=OAS())),\n",
    "#         (\"nys\", Nystroem(random_state=random_state, n_jobs=-1, gamma=0.010, n_components=1000)),\n",
    "#     ]\n",
    "# )\n",
    "# preprocess_name = \"MinMax+LDA+NYS\"\n",
    "\n",
    "pipeline.fit(df_X[train_idx], df_y[train_idx])\n",
    "\n",
    "df_X_t = pipeline.transform(df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinations size: 2\n"
     ]
    }
   ],
   "source": [
    "hidden_size = [256]  # 128, 256, 512, 1024]\n",
    "\n",
    "batch_size = [32]  # [8, 16, 32, 64]\n",
    "\n",
    "# dropout_p = [0.2, 0.3]  # 0.2\n",
    "\n",
    "depth = [4, 5]  # [3, 4, 5]\n",
    "\n",
    "learning_rate = [0.05]  # [0.001, 0.1]\n",
    "\n",
    "num_epochs = [200]\n",
    "\n",
    "# It is mathematically the preferred loss function under the inference framework of maximum likelihood if the distribution of the target variable is Gaussian.\n",
    "criterion = [nn.MSELoss()]\n",
    "## provare anche\n",
    "# Mean Absolute Error (MAE) -> less sensitive to outliers\n",
    "# Huber Loss -> combination of MAE\n",
    "# Log-Cosh Loss -> smooth approximation of the MAE loss function. It is less sensitive to outliers than MSE and more robust than MAE.\n",
    "\n",
    "## QUALE OPTIMIZER USIAMO??\n",
    "# optimizer = torch.optim.SGD\n",
    "optimizer = [torch.optim.Adam]\n",
    "# Momentum\n",
    "# RMSprop\n",
    "\n",
    "hyperparameters = list(\n",
    "    product(\n",
    "        hidden_size,\n",
    "        depth,\n",
    "        num_epochs,\n",
    "        batch_size,\n",
    "        learning_rate,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        # step_size_lr_decay,\n",
    "    )\n",
    ")\n",
    "n_comb = len(hyperparameters)\n",
    "print(f\"Combinations size: {n_comb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "Iteration 1/2\n",
      "RAW--Linear(all same size)+ReLU--depth=4--hidden_size=256--batch_size=32--lr=0.05\n",
      "NeuralNetwork(\n",
      "  (input_layer): Sequential(\n",
      "    (0): Linear(in_features=90, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (output): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "best_loss = np.inf\n",
    "best_combination = None\n",
    "best_model = None\n",
    "\n",
    "results_tt = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"preprocess\",\n",
    "        \"NN_architecture\",\n",
    "        \"depth\",\n",
    "        \"hidden_size\",\n",
    "        \"batch_size\",\n",
    "        \"learning_rate\",\n",
    "        \"num_epochs\",\n",
    "        \"criterion\",\n",
    "        \"optimizer\",\n",
    "        \"R^2\",\n",
    "        \"MSE\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for n_iter, (hidden_size, depth, num_epochs, batch_size, learning_rate, criterion, optimizer) in enumerate(\n",
    "    hyperparameters\n",
    "):\n",
    "    fix_random(random_state)\n",
    "    print(\"===========================\")\n",
    "    print(f\"Iteration {n_iter+1}/{n_comb}\")\n",
    "\n",
    "    # Data Loaders\n",
    "    my_dataset = MyDataset(df_X_t, df_y)\n",
    "    train_loader = DataLoader(Subset(my_dataset, train_idx), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(Subset(my_dataset, val_idx), batch_size=1)\n",
    "    test_loader = DataLoader(Subset(my_dataset, test_idx), batch_size=1)\n",
    "\n",
    "    model = NeuralNetwork(my_dataset.num_features, hidden_size, depth=depth)\n",
    "    model.to(device)\n",
    "\n",
    "    log_name = (\n",
    "        preprocess_name\n",
    "        + \"--\"\n",
    "        + model._get_description()\n",
    "        + \"--\"\n",
    "        + f\"depth={depth}--hidden_size={hidden_size}--batch_size={batch_size}--lr={learning_rate}\"\n",
    "    )\n",
    "    print(log_name)\n",
    "    print(model)\n",
    "    log_writer = SummaryWriter(runs_dir + log_name)\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer(model.parameters(), lr=learning_rate),\n",
    "        num_epochs,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        log_writer,\n",
    "        log_name,\n",
    "    )\n",
    "\n",
    "    # Load best model (saven in the train function)\n",
    "    model.load_state_dict(torch.load(model_dir + log_name))\n",
    "    model.to(device)\n",
    "\n",
    "    # Test\n",
    "    y_true, y_pred = test_model(model, test_loader, device)\n",
    "    test_loss = criterion(y_pred, y_true)\n",
    "    print(f\"Test loss: {test_loss:.4f}\")\n",
    "    mse = mean_squared_error(y_true.detach().numpy(), y_pred.detach().numpy())\n",
    "    r2 = r2_score(y_true.cpu().detach().numpy(), y_pred.cpu().detach().numpy())\n",
    "    print(f\"R^2: {r2:.4f}, MSE:{mse:.4f}\")\n",
    "\n",
    "    results_tt.loc[len(results_tt)] = [\n",
    "        preprocess_name,\n",
    "        model._get_description(),\n",
    "        depth,\n",
    "        hidden_size,\n",
    "        batch_size,\n",
    "        learning_rate,\n",
    "        num_epochs,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        r2,\n",
    "        mse,\n",
    "    ]\n",
    "\n",
    "    if test_loss < best_loss:\n",
    "        best_model = model\n",
    "        best_combination = results_tt.loc[len(results_tt) - 1]\n",
    "\n",
    "    log_writer.flush()\n",
    "    log_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combination:\n",
      "preprocess                                     RAW\n",
      "NN_architecture         Linear(all same size)+ReLU\n",
      "depth                                            3\n",
      "hidden_size                                    256\n",
      "batch_size                                      32\n",
      "learning_rate                                  0.1\n",
      "num_epochs                                     200\n",
      "criterion                                MSELoss()\n",
      "optimizer          <class 'torch.optim.adam.Adam'>\n",
      "R^2                                      -0.000001\n",
      "MSE                                     110.173683\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Best combination:\")\n",
    "print(best_combination)\n",
    "\n",
    "results_tt.to_csv(\"2_output.csv\", mode=\"a\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocess</th>\n",
       "      <th>NN_architecture</th>\n",
       "      <th>depth</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>criterion</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>R^2</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RAW</td>\n",
       "      <td>Linear(all same size)+ReLU</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>MSELoss()</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>-9.920416e-07</td>\n",
       "      <td>1.101737e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RAW</td>\n",
       "      <td>Linear(all same size)+ReLU</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>MSELoss()</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>-1.167210e-06</td>\n",
       "      <td>1.101737e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RAW</td>\n",
       "      <td>Linear(all same size)+ReLU</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>8</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>MSELoss()</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>-1.022644e-05</td>\n",
       "      <td>1.101747e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RAW</td>\n",
       "      <td>Linear(all same size)+ReLU</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>MSELoss()</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>-1.044225e-05</td>\n",
       "      <td>1.101747e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RAW</td>\n",
       "      <td>Linear(all same size)+ReLU</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>MSELoss()</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>-1.044225e-05</td>\n",
       "      <td>1.101747e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RAW</td>\n",
       "      <td>Linear(all same size)+ReLU</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>MSELoss()</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>-3.133208e-04</td>\n",
       "      <td>1.102081e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MinMax+LDA+NYS</td>\n",
       "      <td>Linear(all same size)+ReLU</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>MSELoss()</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>-4.358933e-03</td>\n",
       "      <td>1.106538e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MinMax+Lmax+LDA</td>\n",
       "      <td>Linear(all same size)+ReLU</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>MSELoss()</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>-7.474897e-03</td>\n",
       "      <td>1.109971e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RAW</td>\n",
       "      <td>Linear(all same size)+ReLU</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>MSELoss()</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>-3.010966e-02</td>\n",
       "      <td>3.953644e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RAW</td>\n",
       "      <td>Linear(all same size)+ReLU</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>8</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>MSELoss()</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>-9.324554e+03</td>\n",
       "      <td>1.027430e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        preprocess             NN_architecture  depth  hidden_size  \\\n",
       "9              RAW  Linear(all same size)+ReLU      3          256   \n",
       "7              RAW  Linear(all same size)+ReLU      2          256   \n",
       "8              RAW  Linear(all same size)+ReLU      3          256   \n",
       "1              RAW  Linear(all same size)+ReLU      2          128   \n",
       "2              RAW  Linear(all same size)+ReLU      3          128   \n",
       "3              RAW  Linear(all same size)+ReLU      3          128   \n",
       "5   MinMax+LDA+NYS  Linear(all same size)+ReLU      3          128   \n",
       "4  MinMax+Lmax+LDA  Linear(all same size)+ReLU      3          128   \n",
       "0              RAW  Linear(all same size)+ReLU      1          128   \n",
       "6              RAW  Linear(all same size)+ReLU      2          256   \n",
       "\n",
       "   batch_size  learning_rate  num_epochs  criterion  \\\n",
       "9          32           0.10         200  MSELoss()   \n",
       "7          32           0.10         200  MSELoss()   \n",
       "8           8           0.10         200  MSELoss()   \n",
       "1          32           0.10         200  MSELoss()   \n",
       "2          32           0.10         200  MSELoss()   \n",
       "3          32           0.01         200  MSELoss()   \n",
       "5          32           0.10         200  MSELoss()   \n",
       "4          32           0.10         200  MSELoss()   \n",
       "0          32           0.10         200  MSELoss()   \n",
       "6           8           0.10         200  MSELoss()   \n",
       "\n",
       "                         optimizer           R^2           MSE  \n",
       "9  <class 'torch.optim.adam.Adam'> -9.920416e-07  1.101737e+02  \n",
       "7  <class 'torch.optim.adam.Adam'> -1.167210e-06  1.101737e+02  \n",
       "8  <class 'torch.optim.adam.Adam'> -1.022644e-05  1.101747e+02  \n",
       "1  <class 'torch.optim.adam.Adam'> -1.044225e-05  1.101747e+02  \n",
       "2  <class 'torch.optim.adam.Adam'> -1.044225e-05  1.101747e+02  \n",
       "3  <class 'torch.optim.adam.Adam'> -3.133208e-04  1.102081e+02  \n",
       "5  <class 'torch.optim.adam.Adam'> -4.358933e-03  1.106538e+02  \n",
       "4  <class 'torch.optim.adam.Adam'> -7.474897e-03  1.109971e+02  \n",
       "0  <class 'torch.optim.adam.Adam'> -3.010966e-02  3.953644e+03  \n",
       "6  <class 'torch.optim.adam.Adam'> -9.324554e+03  1.027430e+06  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"2_output.csv\").sort_values(by=\"R^2\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 110.17368\n",
      "R2: -7393631224.120496\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_true = test_model(best_model, test_loader, device)\n",
    "\n",
    "mse = mean_squared_error(y_true.detach().numpy(), y_pred.detach().numpy())\n",
    "r2 = r2_score(y_true.detach().numpy(), y_pred.detach().numpy())\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2:\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
